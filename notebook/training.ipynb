{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95da9b5b-6eef-47e4-9ca1-9e75d261481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms, models, datasets\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a391e655-f081-48b1-ad0c-e57dd5b8d58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = os.path.join('../data/train')\n",
    "VALID_DIR = os.path.join('../data/valid')\n",
    "BATCH_SIZE = 10\n",
    "IMAGE_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45256c00-b96f-42d9-945a-6a0ea37e6bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transforms.Compose([transforms.Resize(IMAGE_SIZE),\n",
    "                                  transforms.RandomRotation(degrees=20),\n",
    "                                  transforms.ToTensor()])\n",
    "train_set = torchvision.datasets.ImageFolder(root=TRAIN_DIR, transform=transformer)\n",
    "valid_set = torchvision.datasets.ImageFolder(root=VALID_DIR, transform=transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "955eb3c9-772f-47a4-a137-18c3fb992050",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_set,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "182063c4-5322-4156-b414-4b4e0b7f5804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SqueezeNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 100, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (3): Fire(\n",
       "      (squeeze): Conv2d(100, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Fire(\n",
       "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Fire(\n",
       "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (7): Fire(\n",
       "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Fire(\n",
       "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): Fire(\n",
       "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Fire(\n",
       "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (12): Fire(\n",
       "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Conv2d(512, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squeezenet = models.squeezenet1_0(num_classes=4)\n",
    "squeezenet.features[0] = nn.Conv2d(3, 100, kernel_size=(3, 3), stride=(2, 2))\n",
    "squeezenet.features[3].squeeze = nn.Conv2d(100, 16, kernel_size=(1, 1), stride=(1, 1))\n",
    "squeezenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97fb5ef6-d3b3-4955-9d14-adecfbaf29a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(model, criterion, optimizer, trainloader, validloader, epochs=5):\n",
    "\n",
    "    train_loss = []\n",
    "    valid_loss = []\n",
    "    train_acc = []\n",
    "    valid_acc = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        train_batch_loss = 0\n",
    "        valid_batch_loss = 0\n",
    "        train_batch_acc = 0\n",
    "        valid_batch_acc= 0\n",
    "\n",
    "        for X, y in trainloader:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_batch_loss += loss.item()\n",
    "            y_label = y_pred.argmax(dim=1)\n",
    "            train_batch_acc += sum(y == y_label).type(torch.float32) / len(y)\n",
    "\n",
    "        train_loss.append(train_batch_loss / len(trainloader))\n",
    "        train_acc.append(train_batch_acc / len(trainloader))\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for X_valid, y_valid in validloader:\n",
    "                y_pred = model(X_valid)\n",
    "                loss = criterion(y_pred, y_valid)\n",
    "                y_label = y_pred.argmax(dim=1)\n",
    "                valid_batch_loss += loss.item()\n",
    "                valid_batch_acc += sum(y_valid == y_label).double() / len(y_valid)\n",
    "\n",
    "            valid_loss.append(valid_batch_loss / len(validloader))\n",
    "            valid_acc.append(valid_batch_acc / len(validloader))\n",
    "        model.train()\n",
    "        \n",
    "        \n",
    "\n",
    "        print(f\"\"\"Epoch #{epoch+1}, Train loss: {train_loss[-1]:.2f}, Train Acc: {train_acc[-1]:.2f},\n",
    "          Valid loss: {valid_loss[-1]:.2f}, Valid Acc: {valid_acc[-1]:.2f}\"\"\")\n",
    "    return train_acc, valid_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfd0da1f-ddd6-4a33-9f8b-940c01b22cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "CRITERION = nn.CrossEntropyLoss()\n",
    "OPTIMIZER = optim.Adam(squeezenet.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60e121ec-ac44-40a3-9f05-efb2660f93a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1, Train loss: 1.35, Train Acc: 0.26,\n",
      "          Valid loss: 1.38, Valid Acc: 0.35\n",
      "Epoch #2, Train loss: 1.01, Train Acc: 0.50,\n",
      "          Valid loss: 0.93, Valid Acc: 0.51\n",
      "Epoch #3, Train loss: 0.88, Train Acc: 0.58,\n",
      "          Valid loss: 1.03, Valid Acc: 0.51\n",
      "Epoch #4, Train loss: 1.02, Train Acc: 0.50,\n",
      "          Valid loss: 1.03, Valid Acc: 0.49\n",
      "Epoch #5, Train loss: 1.03, Train Acc: 0.50,\n",
      "          Valid loss: 1.04, Valid Acc: 0.49\n",
      "Epoch #6, Train loss: 1.02, Train Acc: 0.50,\n",
      "          Valid loss: 1.02, Valid Acc: 0.49\n",
      "Epoch #7, Train loss: 1.01, Train Acc: 0.51,\n",
      "          Valid loss: 0.99, Valid Acc: 0.51\n",
      "Epoch #8, Train loss: 0.95, Train Acc: 0.52,\n",
      "          Valid loss: 0.80, Valid Acc: 0.68\n",
      "Epoch #9, Train loss: 0.57, Train Acc: 0.69,\n",
      "          Valid loss: 0.54, Valid Acc: 0.77\n",
      "Epoch #10, Train loss: 0.20, Train Acc: 0.92,\n",
      "          Valid loss: 0.34, Valid Acc: 0.83\n",
      "Epoch #11, Train loss: 0.02, Train Acc: 0.99,\n",
      "          Valid loss: 0.02, Valid Acc: 0.99\n",
      "Epoch #12, Train loss: 0.00, Train Acc: 1.00,\n",
      "          Valid loss: 0.04, Valid Acc: 0.98\n",
      "Epoch #13, Train loss: 0.00, Train Acc: 1.00,\n",
      "          Valid loss: 0.01, Valid Acc: 1.00\n",
      "Epoch #14, Train loss: 0.22, Train Acc: 0.93,\n",
      "          Valid loss: 0.05, Valid Acc: 0.99\n",
      "Epoch #15, Train loss: 0.04, Train Acc: 0.99,\n",
      "          Valid loss: 0.13, Valid Acc: 0.97\n",
      "Epoch #16, Train loss: 0.04, Train Acc: 0.98,\n",
      "          Valid loss: 0.03, Valid Acc: 0.99\n",
      "Epoch #17, Train loss: 0.00, Train Acc: 1.00,\n",
      "          Valid loss: 0.03, Valid Acc: 0.99\n",
      "Epoch #18, Train loss: 0.00, Train Acc: 1.00,\n",
      "          Valid loss: 0.02, Valid Acc: 0.99\n",
      "Epoch #19, Train loss: 0.00, Train Acc: 1.00,\n",
      "          Valid loss: 0.02, Valid Acc: 0.99\n",
      "Epoch #20, Train loss: 0.00, Train Acc: 1.00,\n",
      "          Valid loss: 0.02, Valid Acc: 0.99\n"
     ]
    }
   ],
   "source": [
    "trainer(squeezenet, CRITERION, OPTIMIZER, train_loader, valid_loader, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8bfbe3e-59af-453b-a219-759bf815af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"model.pt\"\n",
    "model_save_path = f\"../model/{MODEL_NAME}\"\n",
    "torch.save(squeezenet.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638a4303-466f-4c74-aa6a-da76d7de3bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:575]",
   "language": "python",
   "name": "conda-env-575-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
